{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementary Problem Set\n",
    "原始文档链接：<http://cs224d.stanford.edu/assignment1/assignment1.pdf>\n",
    "\n",
    "### 1. Softmax\n",
    "softmax的准确定义，[Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)上写的很清楚：\n",
    ">In mathematics, in particular probability theory and related fields, the softmax function, or normalized exponential, is a generalization of the logistic function that \"squashes\" a K-dimensional vector $\\mathbf {z} $ of arbitrary real values to a K-dimensional vector $\\sigma (\\mathbf {z} )$ of real values in the range (0, 1) that add up to 1. \n",
    "\n",
    "那么，根据定义，对于一个$\\ n\\ $维向量$\\ \\vec x\\ $，向量$\\ \\vec x\\ $中的每一个元素$\\ x_i$，将会变成\n",
    "$$ P(x_i)= \\frac{e ^ {x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "softmax的结果可以解释为概率，因此这里用$P(x_i)$表示。\n",
    "那么，若$\\ \\vec x_i' = \\vec x + 3\\ $，那么对于每一个元素$x_i'$，有\n",
    "$$P(x_i') = \\frac{e^{x{_i}'} }{\\sum_{j=1}^{n}e^{x_j'}} \\\\\n",
    "\t\t= \\frac{e^{x{_i} + 3} }{\\sum_{j=1}^{n}e^{x_j + 3}} \\\\\n",
    "\t\t= \\frac{e^{x{_i}} * e ^ 3}{e^3 * \\sum_{j=1}^{n}e^{x_j + 3}} \\\\\n",
    "\t\t= \\frac{e ^ {x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "给的材料里写了：\n",
    ">Note: In practice, we make use of this property and choose $c = − max_i\\{x_i\\}$ when computing softmax probabilities for numerical stability (i.e. subtracting its maximum element from all elements of x).\n",
    "\n",
    "### 2. Neural Network Basics\n",
    "\n",
    "#### (a)\n",
    "$$\\sigma'(z) = (\\frac{1}{1 + e ^{-z}})' \\\\\n",
    "= - (\\frac{1}{1 + e ^ {-z}}) ^ 2 * (-e^{-z})  \\\\\n",
    "= \\frac{1}{1 + e ^ {-z}} * \\frac{e ^ {-z}}{1 + e ^ {-z}} \\\\\n",
    "= \\sigma(z) * (1-\\sigma(z))$$\n",
    "\n",
    "#### (b)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial CE(\\vec y , \\hat{\\vec y})}{\\partial {\\vec \\theta}} = \n",
    "\\frac{-\\sum_i^m y_i * log(\\hat y_i)}{\\partial {\\vec \\theta}} =  ①$$\n",
    "\n",
    "\n",
    "$$ \\text{assume} \\   y_i = \n",
    "\\begin{cases}\n",
    "1,  & \\text{if $i$ = k} \\\\[2ex]\n",
    "0, & \\text{if $i \\ != k$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "① = \\frac{-log(y_k)}{\\partial {\\vec \\theta}} = \\frac{-log(\\frac{e^{\\theta_k}}{\\sum_{x=1}^m e ^ {\\theta_x}})}{\\partial {\\vec \\theta}} \\\\\n",
    "= \\frac{-\\theta_k + log(\\sum_{x=1}^m e ^ {\\theta_x})}{\\partial {\\vec \\theta}} =\n",
    "②\n",
    "$$\n",
    "\n",
    "$$\\text{if i != k,  for each $\\theta_i$, we have } ②\n",
    "=  \\frac{\\partial(-\\theta_k)}{\\partial \\theta_i} + \\frac{\\partial log(\\sum_{x=1}^m e ^ {\\theta_x})}{\\partial \\theta _i} \n",
    "= \\frac{\\partial(-\\theta_k)}{\\partial \\theta_i} + \\frac{e^{\\theta_i}}{\\sum_{x=1}^m e ^ {\\theta_x}} = 0 + \\hat y_i =\\hat  y_i\n",
    "\\text{, }\n",
    "$$\n",
    "\n",
    "$$ \\text{if i != k,  apparently we have ② } = \\hat y_i - 1\n",
    "\\text{, }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{thus, }\\frac{\\partial CE(\\vec y , \\hat{\\vec y})}{\\partial {\\vec \\theta}} = \\hat {\\vec y} - \\vec y \n",
    "$$\n",
    "\n",
    "#### (c)\n",
    "我是按$\\vec x, \\vec y, \\vec h$都是列向量来做的。\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial x} = W^{(1)} * \\delta^{(2)} \\\n",
    "= W^{(1)} * ((W^{(2)} * (\\hat{\\vec y} - \\vec y)) \\circ (\\vec h \\circ (1 - \\hat h))) \n",
    "$$\n",
    "验证维度\n",
    "$$W^{(1)} \\in D_x * H, \\\n",
    "W^{(2)} \\in H * D_y, \\\n",
    "\\vec y \\in D_y * 1, \\\n",
    "\\vec h \\in H * 1,\n",
    "\\frac{\\partial J}{\\partial x} \\in D_x * 1$$\n",
    "\n",
    "=======\n",
    "编程的时候按题意重新梳理了一遍。\n",
    "\n",
    "forward\n",
    "$$\n",
    "\\mathbf z^{(1)} = \\mathbf {x} \\\\\n",
    "\\mathbf a^{(1)} = \\mathbf {x} \\\\\n",
    "\\mathbf z^{(2)} = \\mathbf a^{(1)} \\bullet W ^ {(1)} + \\mathbf b^{(1)} \\\\\n",
    "\\mathbf a^{(2)} = sigmoid(\\mathbf z^{(2)}) = \\mathbf h \\\\\n",
    "\\mathbf z^{(3)} = \\mathbf a^{(2)} \\bullet W ^{(2)} + \\mathbf b^{(2)} = \\mathbf \\theta\\\\\n",
    "\\mathbf a ^{(3)} = softmax(\\mathbf z^{(3)})= \\hat{\\mathbf y} \n",
    "$$\n",
    "backward\n",
    "$$\n",
    "\\delta^{(l)} = \\frac{\\partial J}{\\partial z^{(l)}} \\\\\n",
    "\\delta^{(3)} = \\mathbf a^{(3)} - \\mathbf y \\\\\n",
    "\\frac{\\partial J}{\\partial W^{(2)}} = \\mathbf a^{(2)T} \\bullet \\delta^{(3)}\\\\\n",
    "\\text{其他的看代码理解吧，懒得打了}\n",
    "$$\n",
    "一定要理解[第五节课](http://blog.csdn.net/jasonwayne/article/details/50484572)中总结的两个式子\n",
    "\n",
    "\n",
    "#### (d)\n",
    "$$(D_x * H + H) + (H * D_y + D_y) $$\n",
    "\n",
    "### 3. word2vec\n",
    "\n",
    "#### (a)\n",
    "$ J = - \\sum p log q $, 其中p是one hot vector，只有正确词是1，q是softmax求得的probability，因此，有\n",
    "$$ \\frac{\\partial J}{\\partial \\hat{\\vec r}} = \\frac{\\partial (-1 * log(\\frac{exp(\\vec w_i^T,  \\hat {\\vec r} )}{\\sum_{j=1}^{|v|} exp(\\vec w_j^T \\bullet \\hat{\\vec r})}))}{\\partial \\hat{\\vec r}}\n",
    "\\\\\n",
    "= - \\vec w_i + \\frac{\\partial (log(\\sum_{j=1}^{|v|} exp(\\vec w_j^T \\bullet \\hat{\\vec r})))}{\\partial \\hat{\\vec r}} \\\\\n",
    "= - \\vec w_i + \\frac{1}{\\sum_{j=1}^{|V|} exp(\\vec w_j^T \\bullet \\hat {\\vec r} )} * \\sum_{x=1}^{|V|} exp(\\vec w_x^T,   \\hat{\\vec r}) * \\vec w_x \\\\\n",
    "= - \\vec w_i + \\sum_{x=1}^{|V|}Pr(word_x|\\ \\vec w,  \\hat {\\vec r}) * \\vec w_x\n",
    "$$\n",
    "\n",
    "#### (b)\n",
    "和前面的推导几乎一样，注意原式分子的sum只会剩下一项，因此，最终结果中的sum也只剩下一项；另外，$w_i$当i是expected word时，才会留下前面的$-\\hat {\\vec r}$\n",
    "\n",
    "#### (c)\n",
    "$$ \\require{cancel}\n",
    "\\frac{\\partial J}{\\partial \\hat {\\vec r}} = -\\frac{1}{\\cancel{\\sigma ()}} * \\cancel{\\sigma()} * (1-\\sigma()) * \\vec w_i - \\sum_{k=1}^K \\frac{1}{\\sigma()} * \\sigma() * (1-\\sigma) * (-\\vec {w_k}) \\\\\n",
    "= (\\sigma(w_i^T \\bullet \\hat{\\vec r}) - 1)* \\vec w_i + \\sum_{k=1}^K (1-\\sigma(-w_k^T \\bullet \\hat{\\vec r})) \\vec w_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "j = \\text{expected word i, we have} \n",
    "\\frac{\\partial J}{\\partial w_i} = (\\sigma(w_i^T \\bullet \\hat{\\vec r}) - 1) * \\hat{\\vec r} \\\\\n",
    "j != i \\text{and j in K}, \\frac{\\partial J}{\\partial w_j} = (1-\\sigma(-w_j^T \\bullet \\hat{\\vec r})) * \\hat{\\vec r} \\\\\n",
    "\\text{j != i and j not in K}, \\frac{\\partial J}{\\partial w_j} =0\n",
    "$$\n",
    "\n",
    "#### (d)\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf {v_{w_i}}} = \\sum _{-c \\le j \\le C, j \\ne c} \\frac{\\partial F(\\mathbf v'_{w_{i+j}}, \\mathbf v_{w_i})}{\\partial \\mathbf {v_{w_i}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf {v'_{w_{i+j}}}} = \\frac{\\partial F(\\mathbf v'_{w_{i+j}}, \\mathbf v_{w_i})}{\\partial \\mathbf {v'_{w_{i+j}}}} \\text{for $ -c \\le j \\le c$ and $ j \\ne 0$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
